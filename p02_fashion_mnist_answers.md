# deep-learning-jhu-cs-482-682
## Names<br>
Michael Chan<br>
Chenglin Yang<br>
Kuo-Wei Lai<br>
## Answers<br>
### Question1: Compare the performance of mnist and fashion-mnist<br>
#### MNIST default arguments<br>
best validation accuracy: 0.9759<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.51035|1.4627239945411683|0.8889|0.42414556884765625|
|2.0|0.8295833333333333|0.5407302761077881|0.9279|0.2418047348022461|
|3.0|0.8821333333015442|0.3889531023184458|0.9473|0.1737140655517578|
|4.0|0.9042999999682109|0.3149789578994115|0.9577|0.13951731491088867|
|5.0|0.9180833333333334|0.2728260396798452|0.9621|0.11927547302246094|
|6.0|0.9274166666984558|0.2419294715722402|0.9668|0.10409429779052734|
|7.0|0.9332333333333334|0.22203802625338237|0.971|0.09342096939086914|
|8.0|0.9390999999682108|0.20274044160048166|0.9731|0.08513947982788087|
|9.0|0.9441999999682108|0.18803671177228293|0.9755|0.07988546752929687|
|10.0|0.9458166666348775|0.18145897832711538|0.9759|0.07658514213562012|

#### Fashion-MNIST default arguments<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

**[Tensorboard plot]**<br>
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q1_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q1_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q1_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q1_val_loss.png)<br>
**[Comment for Q1]<br>**
The converged accuracy is higher and the converged loss is lower for mnist dataset. We can conclude that mnist dataset is easier to classify with the neural network we used.<br>

### Question2: Train for twice as many epochs for both mnist and fashion_mnist.<br>
#### MNIST 10epochs<br>
best validation accuracy: 0.9759<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.51035|1.4627239945411683|0.8889|0.42414556884765625|
|2.0|0.8295833333333333|0.5407302761077881|0.9279|0.2418047348022461|
|3.0|0.8821333333015442|0.3889531023184458|0.9473|0.1737140655517578|
|4.0|0.9042999999682109|0.3149789578994115|0.9577|0.13951731491088867|
|5.0|0.9180833333333334|0.2728260396798452|0.9621|0.11927547302246094|
|6.0|0.9274166666984558|0.2419294715722402|0.9668|0.10409429779052734|
|7.0|0.9332333333333334|0.22203802625338237|0.971|0.09342096939086914|
|8.0|0.9390999999682108|0.20274044160048166|0.9731|0.08513947982788087|
|9.0|0.9441999999682108|0.18803671177228293|0.9755|0.07988546752929687|
|10.0|0.9458166666348775|0.18145897832711538|0.9759|0.07658514213562012|

#### MNIST 20epochs<br>
best validation accuracy: 0.9832<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.51035|1.4627239945411683|0.8889|0.42414556884765625|
|2.0|0.8295833333333333|0.5407302761077881|0.9279|0.2418047348022461|
|3.0|0.8821333333015442|0.3889531023184458|0.9473|0.1737140655517578|
|4.0|0.9042999999682109|0.3149789578994115|0.9577|0.13951731491088867|
|5.0|0.9180833333333334|0.2728260396798452|0.9621|0.11927547302246094|
|6.0|0.9274166666984558|0.2419294715722402|0.9668|0.10409429779052734|
|7.0|0.9332333333333334|0.22203802625338237|0.971|0.09342096939086914|
|8.0|0.9390999999682108|0.20274044160048166|0.9731|0.08513947982788087|
|9.0|0.9441999999682108|0.18803671177228293|0.9755|0.07988546752929687|
|10.0|0.9458166666348775|0.18145897832711538|0.9759|0.07658514213562012|
|11.0|0.9492333333015441|0.16944978431463242|0.978|0.07316049079895019|
|12.0|0.9517999999682109|0.15949364439646402|0.9791|0.06992426033020019|
|13.0|0.9535166666348776|0.15546560228268305|0.9794|0.06861714477539063|
|14.0|0.9557666666984558|0.14669645246664684|0.9808|0.06509506874084472|
|15.0|0.95675|0.14437329112291336|0.9808|0.06235464363098144|
|16.0|0.9574333333333334|0.13999996976057688|0.9807|0.06107773933410644|
|17.0|0.95955|0.1347785317103068|0.9826|0.05694325523376465|
|18.0|0.9602666666666667|0.12983395940462747|0.9819|0.05873825187683106|
|19.0|0.9617833333015442|0.12821097732782363|0.9823|0.06003257598876953|
|20.0|0.9623166666984558|0.12504138869047166|0.9832|0.05392145729064941|

#### Fashion-MNIST 10 epochs<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

#### Fashion-MNIST 20 epochs<br>
best validation accuracy: 0.8537<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|
|11.0|0.8000500000317892|0.5607631398200988|0.8258|0.47213839111328126|
|12.0|0.8053166666348776|0.5452092870394388|0.8324|0.4640594909667969|
|13.0|0.8088666666666666|0.5364550201892853|0.8348|0.4529643859863281|
|14.0|0.8139666666984559|0.5249718839963278|0.8371|0.44789151611328126|
|15.0|0.8166833333333333|0.5149602099736531|0.8432|0.4324026275634766|
|16.0|0.8203833333651225|0.5037822475115458|0.8419|0.4308385955810547|
|17.0|0.8233166666348776|0.49949061136245726|0.8424|0.43627376403808593|
|18.0|0.8260833333651225|0.48899531242052713|0.8461|0.41589852600097654|
|19.0|0.8278999999682108|0.48217640115420024|0.8517|0.40811463623046873|
|20.0|0.8292833333015441|0.47585853974024456|0.8537|0.41068582763671874|

**[Tensorboard plot]**<br>
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q2_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q2_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q2_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q2_val_loss.png)<br>
**[Comment for Q2]<br>**
with more epochs, accuracy is higher and loss is lower.<br>

### Question3: Change the SGD Learning Rate by a factor of [0.1x, 1x, 10x]<br>
#### Fashion-MNIST Learning Rate 0.1x<br>
best validation accuracy: 0.7128<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.15721666666666667|2.2699311171213785|0.1788|2.2303862548828124|
|2.0|0.2154166666587194|2.1815693247477213|0.3|2.0948861572265627|
|3.0|0.31178333331743874|1.9976464904149374|0.4439|1.797110986328125|
|4.0|0.3984833333492279|1.7096238748550414|0.5801|1.4424883178710937|
|5.0|0.48536666666666667|1.4412196718215942|0.6289|1.1831152465820312|
|6.0|0.5472000000317891|1.2632985212961834|0.6628|1.0411880737304688|
|7.0|0.5874833333333334|1.153631184577942|0.6796|0.9540066589355469|
|8.0|0.6136333333015442|1.080204509162903|0.6946|0.9027905456542968|
|9.0|0.6344166666666666|1.0288827874819437|0.7029|0.8578368957519531|
|10.0|0.6486833333333333|0.9848848899841308|0.7128|0.820594482421875|

#### Fashion-MNIST Learning Rate 1x<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

#### Fashion-MNIST Learning Rate 10x<br>
best validation accuracy: 0.8813<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.6695833333015442|0.8843381965319316|0.7936|0.5544651184082031|
|2.0|0.7886833333651225|0.572480261516571|0.8277|0.4566895263671875|
|3.0|0.82025|0.49984715854326883|0.8351|0.4420435089111328|
|4.0|0.83935|0.45562221786181134|0.8594|0.37528940124511717|
|5.0|0.8486666666666667|0.42677786548932395|0.8635|0.37385331726074217|
|6.0|0.8554833333015441|0.4063305250644684|0.8705|0.35825094299316407|
|7.0|0.8625500000317892|0.3859717491785685|0.8723|0.35014042053222655|
|8.0|0.8659666666984558|0.3761311829884847|0.8699|0.3547170776367187|
|9.0|0.8690666666984558|0.36774855024019876|0.8813|0.32348209838867187|
|10.0|0.8721833333651224|0.3569428035736084|0.8765|0.3406715850830078|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q3_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q3_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q3_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q3_val_loss.png)<br>
**[Comment for Q3]<br>**
lr x10 is the best in terms of the converged loss and accuracy whereas lr x0.1 performs the worst.<br>

### Question4: Compare Optimizers [SGD, Adam, Rmsprop]<br>
#### Fashion-MNIST SGD<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

#### Fashion-MNIST Adam<br>
best validation accuracy: 0.8833<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.6665666666984558|0.9282989700317383|0.7925|0.5413845397949218|
|2.0|0.7803666666666667|0.5993112579027812|0.8243|0.47494334716796877|
|3.0|0.8075999999682109|0.5343259021759034|0.8367|0.43703806762695313|
|4.0|0.8249666666348775|0.4908454336166382|0.8515|0.40068321838378906|
|5.0|0.8359166666984558|0.4607754113356272|0.8573|0.3813578338623047|
|6.0|0.8443500000317892|0.4393839128812154|0.8673|0.36829275817871093|
|7.0|0.8523166666348775|0.4148589918613434|0.868|0.35713517150878904|
|8.0|0.8536833333333333|0.4067981038093567|0.8748|0.33960244140625|
|9.0|0.8596999999682109|0.39467978808085125|0.8792|0.3332452423095703|
|10.0|0.8633499999682108|0.3814853864987691|0.8833|0.3259011291503906|

#### Fashion-MNIST Rmsprop<br>
best validation accuracy: 0.8555<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.7021833333333334|1.2511781585375468|0.759|0.6229300598144532|
|2.0|0.7989666666984558|0.5532118180910747|0.8082|0.5089428833007813|
|3.0|0.8151333333333334|0.5118346834818522|0.8201|0.50620546875|
|4.0|0.82275|0.4850808230717977|0.8021|0.5666925354003907|
|5.0|0.8323166666666667|0.46334372816085817|0.7912|0.6163565551757813|
|6.0|0.8206500000317891|0.551717055384318|0.8236|0.4867425567626953|
|7.0|0.8374499999682109|0.4422959086894989|0.8541|0.3976273864746094|
|8.0|0.8435333333333334|0.4255018507957459|0.8157|0.4662243072509766|
|9.0|0.8455166666666667|0.42081423948605856|0.8555|0.4003697265625|
|10.0|0.8467333333015442|0.41748452838261924|0.8369|0.4451817535400391|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q4_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q4_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q4_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q4_val_loss.png)<br>
**[Comment for Q4]<br>**
In training set, the three optimizers perform relatively similar, sgd is slightly worse than the other two.  In cross validation set, adam outperforms the other two in terms of converged loss and accuracy.<br>

### Question5: Set the dropout layer to a dropout rate of [0, 0.25, 0.5, 0.9, 1]<br>
#### Fashion-MNIST dropout rate 0<br>
best validation accuracy: 0.8466<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.5443999999682109|1.3081305045763651|0.7085|0.7930490661621094|
|2.0|0.7448666666348776|0.6801957058906555|0.7691|0.6269337158203125|
|3.0|0.7821333333333333|0.589542598660787|0.7575|0.6285208923339843|
|4.0|0.8069333333651225|0.5356076414426167|0.799|0.5389427795410157|
|5.0|0.8212833333651225|0.5028495859146118|0.826|0.5033084289550781|
|6.0|0.83245|0.47588431803385417|0.814|0.49950784912109375|
|7.0|0.8400666666666666|0.45691496545473737|0.8271|0.48862855224609375|
|8.0|0.8457000000317891|0.4405245557467143|0.8355|0.46439302368164065|
|9.0|0.8498|0.42367781767845153|0.8466|0.44215118713378904|
|10.0|0.8558666666348775|0.40922972237269084|0.8428|0.44218807067871096|

#### Fashion-MNIST dropout rate 0.25<br>
best validation accuracy: 0.835<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.4951833333651225|1.4190986796061198|0.6981|0.8186527465820312|
|2.0|0.7157999999682109|0.7686257449150086|0.7417|0.6782404296875|
|3.0|0.7529333333651225|0.6667354170481364|0.7607|0.639913232421875|
|4.0|0.7743333333015442|0.6141725148836772|0.7893|0.5588609619140625|
|5.0|0.7900833333333334|0.5765370065371196|0.7878|0.5610337890625|
|6.0|0.7994666666348775|0.5492736957550048|0.8193|0.49728988037109373|
|7.0|0.8128000000317891|0.5224464581807454|0.8213|0.4855038299560547|
|8.0|0.8183166666984558|0.5066681433041891|0.8228|0.49042894897460937|
|9.0|0.8247000000317891|0.49073774466514586|0.8335|0.4607892120361328|
|10.0|0.8302666666348775|0.47628796456654865|0.835|0.46316856384277344|

#### Fashion-MNIST dropout rate 0.5<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

#### Fashion-MNIST dropout rate 0.9<br>
best validation accuracy: 0.769<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.20551666668256124|2.0683516346613566|0.6243|1.4593245849609375|
|2.0|0.3173166666666667|1.677406672668457|0.681|1.1155782836914063|
|3.0|0.3583333333174388|1.574542793782552|0.7074|1.0157705871582032|
|4.0|0.37783333333333335|1.5347718839009603|0.7065|0.9608884704589844|
|5.0|0.3897833333492279|1.5012333916346232|0.7118|0.9085660522460938|
|6.0|0.40236666666666665|1.4742266365051269|0.7515|0.85154208984375|
|7.0|0.41824999998410545|1.43545396677653|0.7561|0.8176701232910156|
|8.0|0.42306666668256127|1.4233575264612834|0.7677|0.8007203979492188|
|9.0|0.4345333333333333|1.39732562084198|0.769|0.7705929626464844|
|10.0|0.43856666666666666|1.3895253175735474|0.7646|0.7690523315429687|

#### Fashion-MNIST dropout rate 1<br>
best validation accuracy: 0.1738<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.09964999999602636|2.3043175078074136|0.145|2.2994537109375|
|2.0|0.09941666666269303|2.303268766784668|0.1529|2.2981404052734375|
|3.0|0.09933333334128062|2.3028661598205566|0.1629|2.2975164794921876|
|4.0|0.09973333332935969|2.302711640548706|0.1717|2.297180224609375|
|5.0|0.09963333333333334|2.302650926208496|0.1738|2.29694814453125|
|6.0|0.09834999999602635|2.3026294034322103|0.1706|2.296844677734375|
|7.0|0.09810000000397365|2.3026224857330324|0.1693|2.2967790283203127|
|8.0|0.09921666666467985|2.3026212787628175|0.1682|2.2967559326171876|
|9.0|0.09858333332935969|2.302616571044922|0.168|2.29670966796875|
|10.0|0.09790000000397364|2.302621120707194|0.1678|2.2967078857421876|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q5_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q5_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q5_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q5_val_loss.png)<br>
**[Comment for Q5]<br>**
In training set, dropout rate of 0 performs the best while dropout rate of 1 performs the worst. Similarly behaviors are observed in cross validation set. It is obvious that as dropout rate approaches 1, model performs degrade significantly. This makes sense because ignoring all inputs (dropout rate of 1) can certainly not help make a good prediction.<br>

### Question6: Change the batch size by a factor of: [1/8x, 1x, 8x]<br>
#### Fashion-MNIST batch size 8<br>
best validation accuracy: 0.8831<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.6774|0.8724159468809763|0.7992|0.5354183166503906|
|2.0|0.7848166666666667|0.5880152640183767|0.8193|0.4653533386230469|
|3.0|0.8156166666666667|0.5146612186590831|0.84|0.42623797912597655|
|4.0|0.8331666666666667|0.46891396374702454|0.8586|0.38408455200195313|
|5.0|0.8428|0.4400544599175453|0.8594|0.37147225036621095|
|6.0|0.8507166666666667|0.4161851139386495|0.8708|0.3473885559082031|
|7.0|0.85845|0.39561644426981607|0.8728|0.3417329864501953|
|8.0|0.8635833333333334|0.3876197799523671|0.8719|0.34044130249023435|
|9.0|0.8650666666666667|0.3754994894146919|0.8813|0.3267203674316406|
|10.0|0.8684|0.36634359807570777|0.8831|0.3197134521484375|

#### Fashion-MNIST batch size 64<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

#### Fashion-MNIST batch size 512<br>
best validation accuracy: 0.7298<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.15894999997615814|2.2658728199005127|0.1857|2.2125542236328126|
|2.0|0.24093333342870077|2.1407154143015545|0.3627|1.9925019775390624|
|3.0|0.35695000001589455|1.8572953837712607|0.5317|1.5527050537109375|
|4.0|0.4602000000635783|1.5065451564153036|0.6307|1.200082763671875|
|5.0|0.5460499997138977|1.257002560297648|0.661|1.0225132690429688|
|6.0|0.5937666663805644|1.1295618393580118|0.6851|0.9331032043457032|
|7.0|0.6265999999682108|1.049500048828125|0.6997|0.8689065124511719|
|8.0|0.6458999997456869|0.9901833113988241|0.7094|0.8311602722167969|
|9.0|0.6622333330472311|0.9495960877100627|0.7204|0.7897306762695312|
|10.0|0.6737500001907348|0.9102226278305053|0.7298|0.7602417785644531|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q6_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q6_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q6_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q6_val_loss.png)<br>
**[Comment for Q6]<br>**
In training set, small batch size performs the best and vice versa. Similar behaviors are observed in cross validation set. Note that if we increase the epochs run for the large batch size such that it runs as many iterations as the small bath size model, it could probably achieve the same performace. This makes sense because batch size is a mean to leverage computational cost. One should find a balance between the robustness of stochastic gradient descent (very small batch size) and the efficiency of batch gradient descent (very large batch size).<br>

### Question7: Change the number of output channels in each convolution and the first Linear layer. [0.5x, 1x, 2x]<br>
#### Fashion-MNIST 0.5x output channel<br>
best validation accuracy: 0.7697<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.37778333333333336|1.662165526008606|0.642|1.0194363891601563|
|2.0|0.5336833333015442|1.1844388463973998|0.6754|0.8555082885742188|
|3.0|0.5713499999682109|1.0857269406636556|0.6966|0.7958349060058594|
|4.0|0.5993666666984558|1.0088763593991597|0.7049|0.7578386047363281|
|5.0|0.6157|0.968144054921468|0.7179|0.7012644592285157|
|6.0|0.6330666666984558|0.9231412434895834|0.726|0.6830896057128907|
|7.0|0.6376666666348775|0.8996257258097331|0.7393|0.6641751098632812|
|8.0|0.6459333333651225|0.8811931038220724|0.7572|0.647257666015625|
|9.0|0.6505333333651224|0.8669891364733379|0.7651|0.6246878967285157|
|10.0|0.6588833333333334|0.8556380607922872|0.7697|0.6126184875488281|

#### Fashion-MNIST 1x output channel<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

#### Fashion-MNIST 2x output channel<br>
best validation accuracy: 0.8433<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.5351000000317891|1.3268392472902935|0.7217|0.7729292114257813|
|2.0|0.7162166666666666|0.7677334999402364|0.7373|0.6613069946289063|
|3.0|0.7476833333015442|0.6763231711069743|0.7803|0.580921826171875|
|4.0|0.7728666666984558|0.619052711836497|0.7923|0.5564350646972657|
|5.0|0.7871333333333334|0.5770642507870992|0.8138|0.5150906677246094|
|6.0|0.8017499999682108|0.5508670418580374|0.8229|0.48790302734375|
|7.0|0.8115666666666667|0.5246224685192108|0.8297|0.4715865997314453|
|8.0|0.8201666666348775|0.5047274751822154|0.8278|0.4604442352294922|
|9.0|0.8250666666666666|0.49200475403467814|0.8211|0.47618006591796874|
|10.0|0.8299500000317891|0.47584341972668964|0.8433|0.4286982452392578|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q7_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q7_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q7_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q7_val_loss.png)<br>
**[Comment for Q7]<br>**
In training set, 2X output channels model performs the best and 1/2X output channels model perfoms the worst. Similar behaviors are observed in cross validation set. Usually when number of output channels are increased, we should be concerned about overfitting. In Q7, doubling the output channels does not show sign of increasing variance, so we can conclude that we did not train a overfitting model by doubling the output channels.<br>

### Question8: Add a Batch Normalization Layer after the first convolution.<br>
#### Fashion-MNIST default arguments<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

#### Fashion-MNIST Add a Batch Normalization Layer<br>
best validation accuracy: 0.8424<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.4691|1.5414478456497192|0.7364|0.7823640869140625|
|2.0|0.7071000000317892|0.8168448264757792|0.7537|0.6291143493652344|
|3.0|0.7428166666984558|0.7004239217758179|0.7793|0.5652661499023437|
|4.0|0.7641000000317891|0.6402260520617167|0.7938|0.5258491119384766|
|5.0|0.7779666666666667|0.6016835730393728|0.8006|0.5033698150634766|
|6.0|0.7882333333015442|0.5720381996631623|0.8115|0.48025938720703126|
|7.0|0.7984166666666667|0.5507491353670756|0.8191|0.46751200866699216|
|8.0|0.8079166666666666|0.5291170152982076|0.8229|0.4544758728027344|
|9.0|0.8153166666348776|0.5114010388374328|0.8366|0.4369621948242188|
|10.0|0.8218666666984558|0.4974816458384196|0.8424|0.41952052307128906|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q8_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q8_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q8_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q8_val_loss.png)<br>
**[Comment for Q8]<br>**
In training set, adding a batch normalization layer does not seem to help. In cross validation set, adding a batch normalization layer helps us get a better accuracy and lower loss. Batch normalization can help speed up training and optimize the activation layer by centering input in the linear regime. <br>

### Question9: Add a Dropout layer immediately after the Batch Normalization from the previous question.<br>
#### Fashion-MNIST default arguments<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

#### Fashion-MNIST Add a Dropout layer immediately after the Batch Normalization<br>
best validation accuracy: 0.8304<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.4476|1.588729084078471|0.7328|0.8114526000976563|
|2.0|0.6884333333333333|0.8793247108777364|0.7615|0.6291724060058593|
|3.0|0.7313166666984559|0.744474428876241|0.7762|0.5723694274902343|
|4.0|0.7506166666666667|0.683287010828654|0.7887|0.536108984375|
|5.0|0.7646166666984558|0.6400547061602274|0.7972|0.5135686462402343|
|6.0|0.7747166666666667|0.616546883392334|0.8033|0.49260137939453125|
|7.0|0.7824499999682109|0.5909874946912129|0.8171|0.4772850830078125|
|8.0|0.7889166666348775|0.5739612432003022|0.8144|0.4632560516357422|
|9.0|0.7991666666348776|0.5545223981062571|0.8304|0.45328138732910156|
|10.0|0.8032333333651225|0.5410005860646566|0.8225|0.4441733642578125|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q9_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q9_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q9_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q9_val_loss.png)<br>
**[Comment for Q9]<br>**
In training set, adding a dropout layer to the model exacerbates the performace. Similar behavior is observed cross validation set. We can conclude that adding a dropout layer immediately after the batch normalization layer may hurt model performace. <br>

### Question10: Move the Batch Normalizaton layer just below the Dropout layer from the previous question.<br>
#### Fashion-MNIST a Dropout layer after the Batch Normalization<br>
best validation accuracy: 0.8304<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.4476|1.588729084078471|0.7328|0.8114526000976563|
|2.0|0.6884333333333333|0.8793247108777364|0.7615|0.6291724060058593|
|3.0|0.7313166666984559|0.744474428876241|0.7762|0.5723694274902343|
|4.0|0.7506166666666667|0.683287010828654|0.7887|0.536108984375|
|5.0|0.7646166666984558|0.6400547061602274|0.7972|0.5135686462402343|
|6.0|0.7747166666666667|0.616546883392334|0.8033|0.49260137939453125|
|7.0|0.7824499999682109|0.5909874946912129|0.8171|0.4772850830078125|
|8.0|0.7889166666348775|0.5739612432003022|0.8144|0.4632560516357422|
|9.0|0.7991666666348776|0.5545223981062571|0.8304|0.45328138732910156|
|10.0|0.8032333333651225|0.5410005860646566|0.8225|0.4441733642578125|

#### Fashion-MNIST a Dropout layer before the Batch Normalization<br>
best validation accuracy: 0.8166<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.3933833333333333|1.7312766318003336|0.7269|0.9097914306640625|
|2.0|0.6652833333333333|0.9459035277366639|0.7546|0.6605055786132813|
|3.0|0.7186666666984558|0.779072234249115|0.7695|0.5950478820800781|
|4.0|0.7414666666348775|0.7047977910677592|0.7822|0.5542380798339843|
|5.0|0.75735|0.6557764059384664|0.7896|0.5289321533203125|
|6.0|0.7682666666666667|0.6292005372683207|0.7942|0.5116734313964844|
|7.0|0.7759833333651225|0.6007029934247334|0.8017|0.49160918884277344|
|8.0|0.78105|0.5844838583151499|0.8042|0.4780649627685547|
|9.0|0.789|0.5652601177056631|0.8166|0.46765877380371096|
|10.0|0.7930666666666667|0.5527122455914816|0.8034|0.4653830474853516|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q10_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q10_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q10_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q10_val_loss.png)<br>
**[Comment for Q10]<br>**
In training set, the two models have similar performance. In cross validation set, adding batch normalization layer immediately after the dropout layer seems to hurt model performace. One should use the model that adds batch normalization layer first if these are the only two models considered.<br>

### Question11: Add one extra Conv2D layer, starting from the default Net model.<br>
#### Fashion-MNIST default arguments<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

#### Fashion-MNIST add one extra Conv2D layer<br>
best validation accuracy: 0.7732<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.15338333333333334|2.253892263285319|0.2873|2.0347436279296875|
|2.0|0.43986666668256125|1.4570773723602295|0.594|1.0922093017578125|
|3.0|0.5982666666984559|1.079081506284078|0.6928|0.8829595825195312|
|4.0|0.6606166666348775|0.9326240899085999|0.7016|0.7954936889648437|
|5.0|0.6914833333333333|0.8531323743184408|0.7341|0.716165234375|
|6.0|0.7103333333651225|0.8063426544189453|0.7309|0.7000230102539062|
|7.0|0.7214666666348776|0.7735148735046387|0.7388|0.6875706726074219|
|8.0|0.7307166666666667|0.7474308929125468|0.7553|0.6495332824707031|
|9.0|0.7383333333651225|0.7246897988319397|0.7575|0.6438798767089844|
|10.0|0.7439833333651225|0.7069063577334086|0.7732|0.6230448547363281|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q11_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q11_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q11_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q11_val_loss.png)<br>
**[Comment for Q11]<br>**
In training set, the model with an extra convolutional layer has a worse perfomance. Similar behavior is observed in cross validation set. One should probably not add an extra convutional layer. A possible explanation for this behavior is that a 3 layers convlutional neural network could distill an image that is too small, which is unable to provide useful information. <br>

### Question12: Remove a layer of your choice, starting from the default Net model.<br>
#### Fashion-MNIST default arguments<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

#### Fashion-MNIST remove a layer<br>
best validation accuracy: 0.8518<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.6127666666666667|1.1157522458076476|0.7572|0.66971298828125|
|2.0|0.7424500000317892|0.728661370754242|0.7941|0.5651449035644531|
|3.0|0.7645|0.6582318935076396|0.8109|0.5376273498535157|
|4.0|0.7797333333651225|0.6196479601224264|0.8185|0.5112514465332031|
|5.0|0.7907333333333333|0.5902319667816162|0.8236|0.48074630432128906|
|6.0|0.8006000000317891|0.5629771562894186|0.8301|0.4633431365966797|
|7.0|0.8084666666348775|0.5459641777356465|0.8405|0.4472247100830078|
|8.0|0.81365|0.5276308051745097|0.8284|0.4523013702392578|
|9.0|0.8183499999682109|0.513402376683553|0.8421|0.42561099853515627|
|10.0|0.8219333333651225|0.5025079755624136|0.8518|0.41346674194335936|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q12_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q12_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q12_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q12_val_loss.png)<br>
**[Comment for Q12]<br>**
In training set, removing a convolutional layer can help improve model performace. Similar behavior is observed in cross validation set. The decision to remove a convolutional layer is based on the result in Q11. Now, we are convinced that perhaps 2 layers is still too many, and 1 convolutional layer is probably the optimal choice.<br>

### Question13: Create the best model you can on Fashion-MNIST based on your experience from the previous questions.<br>
#### Fashion-MNIST default arguments<br>
best validation accuracy: 0.8207<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.44193333330154416|1.5527012502670288|0.6956|0.861683740234375|
|2.0|0.6746333333651224|0.884403501033783|0.751|0.6908711181640625|
|3.0|0.7155999999682109|0.768888208357493|0.7566|0.6359445861816406|
|4.0|0.7384833333015441|0.7123909465789795|0.7754|0.5845791198730469|
|5.0|0.7530666666348775|0.6726672340393066|0.7821|0.5661993225097657|
|6.0|0.7645333333333333|0.6445176569302877|0.7969|0.5356116302490235|
|7.0|0.7742499999682109|0.6168555060704549|0.8005|0.5176242126464844|
|8.0|0.7823833333333333|0.6032948455174764|0.8103|0.5154608795166016|
|9.0|0.7879833333333334|0.5874212433973948|0.8201|0.49306259765625|
|10.0|0.7960166666984558|0.5700705528577169|0.8207|0.49089859619140624|

#### Fashion-MNIST ultimate Fashion-MNIST model<br>
best validation accuracy: 0.9255<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.8531166666666666|0.4079082145770391|0.8878|0.30691576538085935|
|2.0|0.8960666666666667|0.2858702865918477|0.8897|0.2843009201049805|
|3.0|0.909|0.24690859917402266|0.9086|0.25470697479248045|
|4.0|0.9174|0.22383895727793376|0.9018|0.26055814208984374|
|5.0|0.9253166666666667|0.20261060708562534|0.9119|0.23783631439208985|
|6.0|0.9314333333333333|0.18193419230182967|0.9094|0.2609818817138672|
|7.0|0.9361166666666667|0.17096236141721408|0.9091|0.2623898910522461|
|8.0|0.9403|0.16094657307664553|0.9205|0.23938313751220702|
|9.0|0.9464666666666667|0.14221572685639064|0.9196|0.2503077423095703|
|10.0|0.9455166666666667|0.13907132266362507|0.9194|0.26431899719238283|
|11.0|0.9513|0.1303693678001563|0.9106|0.28427888641357424|
|12.0|0.95455|0.12010841541488966|0.9207|0.27518203125|
|13.0|0.9573166666666667|0.11190798606177171|0.9175|0.2896513000488281|
|14.0|0.9609833333333333|0.10542471829652786|0.9157|0.29881016845703123|
|15.0|0.9614333333333334|0.10127409786867599|0.919|0.27715777740478514|
|16.0|0.9626166666666667|0.09624564542596539|0.9227|0.2948377258300781|
|17.0|0.96455|0.09205253176391125|0.9205|0.3090660217285156|
|18.0|0.9664833333333334|0.0876465696938336|0.9243|0.3098487091064453|
|19.0|0.9679|0.08247257571667432|0.9188|0.33330212097167966|
|20.0|0.9692333333333333|0.08139046678245068|0.9255|0.3419825134277344|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q13_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q13_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q13_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q13_val_loss.png)<br>
**[Comment for Q13]<br>**
<br>
### Question14: Evaluate your "ultimate Fashion-MNIST model" by loading the trained weights and running on MNIST without changing the Fashion-MNIST weights at all.<br>
We got the validation accuracy 0.1018
**[Comment for Q14]<br>**
<br>
### Question15: Reduce your SGD learning rate by 20x, and train MNIST on your ultimate Fashion-MNIST model.<br>
#### MNIST default arguments<br>
best validation accuracy: 0.9759<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.51035|1.4627239945411683|0.8889|0.42414556884765625|
|2.0|0.8295833333333333|0.5407302761077881|0.9279|0.2418047348022461|
|3.0|0.8821333333015442|0.3889531023184458|0.9473|0.1737140655517578|
|4.0|0.9042999999682109|0.3149789578994115|0.9577|0.13951731491088867|
|5.0|0.9180833333333334|0.2728260396798452|0.9621|0.11927547302246094|
|6.0|0.9274166666984558|0.2419294715722402|0.9668|0.10409429779052734|
|7.0|0.9332333333333334|0.22203802625338237|0.971|0.09342096939086914|
|8.0|0.9390999999682108|0.20274044160048166|0.9731|0.08513947982788087|
|9.0|0.9441999999682108|0.18803671177228293|0.9755|0.07988546752929687|
|10.0|0.9458166666348775|0.18145897832711538|0.9759|0.07658514213562012|

#### MNIST ultimate Fashion-MNIST model<br>
best validation accuracy: 0.9904<br>

|epoch|acc|loss|val_acc|val_loss|
| --- | --- | --- | --- | --- |
|1.0|0.7328666666666667|1.0419658166011174|0.956|0.14923913803100586|
|2.0|0.9378166666666666|0.20857510592142742|0.9758|0.07721749000549316|
|3.0|0.9639499999682108|0.12355375712712606|0.9824|0.05752531929016113|
|4.0|0.9727166666348775|0.08988093673189482|0.9852|0.04453923244476318|
|5.0|0.9791999999682108|0.06872394775748253|0.9872|0.04115577907562256|
|6.0|0.9831666666348775|0.055469913309812546|0.9883|0.037953177642822265|
|7.0|0.9864|0.04520965737203757|0.9903|0.035080726528167724|
|8.0|0.9876166666666667|0.040228873239209255|0.9898|0.03428639850616455|
|9.0|0.9892499999682108|0.034437305833896|0.9904|0.032110018539428714|
|10.0|0.9900166666666667|0.031615198670576015|0.99|0.036839521408081055|

**[Tensorboard plot]<br>**
train_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q15_train_acc.png)<br>
train_loss:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q15_train_loss.png)<br>
val_acc:<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q15_val_acc.png)<br>
val_loss<br>
![alt text](https://github.com/deep-learning-jhu/p02-fashion-mnist-i-wanna-sleep/blob/master/plot/q15_val_loss.png)<br>
**[Comment for Q15]<br>**
<br>
